{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Multi-task learning in Self eXplainable Deep Neural Networks*\n",
    "==============================================================\n",
    "\n",
    "***Master's thesis - code***\n",
    "\n",
    "*Part 2 - Preparatory Code*\n",
    "\n",
    "**Author:** *Adrian Domaga≈Ça*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparatory Code: Defining Models, Essential Classes, and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains the essential code used throughout the entire study. It includes definitions of model performance metrics, model definitions, functions used for model optimization, functions utilized in Single-Task Learning (STL) and Multi-Task Learning (MTL) approaches, and functions for visualizing the obtained results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import dill\n",
    "import itertools\n",
    "import openml\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss, BCELoss\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import logistic\n",
    "\n",
    "import optuna\n",
    "import dataclasses\n",
    "import joblib\n",
    "from functools import partial\n",
    "from typing import Literal, get_args, Dict\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import lime.lime_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Configurations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to standardize and simplify the definition of functions, I introduced the storage of variables in the form of dataclasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class Folders:\n",
    "    base: str\n",
    "    mtl: str = 'MTL/'\n",
    "    dataset: str = 'Datasets/'\n",
    "    study: str = 'Studies/'\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Files:\n",
    "    data: str\n",
    "    targets: str\n",
    "    study: str\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Paths:\n",
    "    data: str\n",
    "    targets: str\n",
    "    study: str\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Training:\n",
    "    batch_size: int = 64\n",
    "    num_epochs: int = 50\n",
    "    patience: int = 10\n",
    "    early_stopping = True\n",
    "    lr: float = 0.001\n",
    "    optimizer = torch.optim.Adam\n",
    "    epoch_log: int = 5\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Tuning():\n",
    "    name_layers: str = 'num_layers'\n",
    "    max_layers: int = 5\n",
    "    min_layers: int = 1\n",
    "    name_neurons: str = 'num_neurons'\n",
    "    max_neurons: int = None\n",
    "    min_neurons: int = None\n",
    "    sampler = optuna.samplers.TPESampler\n",
    "    num_trials: int = 25\n",
    "\n",
    "@dataclasses.dataclass    \n",
    "class Models():\n",
    "    def __init__(self, base_dir):\n",
    "        self.mlp_opt: str = base_dir + \"mlp_opt_\"\n",
    "        self.reg: str = base_dir + 'reg_'\n",
    "        self.stl_reg: str = base_dir + 'stl_reg_'\n",
    "        self.mtl: str = base_dir + 'mtl_alpha_'\n",
    "        self.ext: str = '.pt'\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Result():\n",
    "    metrics: float\n",
    "    fid: float\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Results():\n",
    "    reg: Dict[int, float] = dataclasses.field(default_factory=dict) \n",
    "    stl_mlp: Dict[int, float] = dataclasses.field(default_factory=dict) \n",
    "    stl_reg: Dict[int, float] = dataclasses.field(default_factory=dict) \n",
    "    mtl: Dict[int, dict] = dataclasses.field(default_factory=dict) \n",
    "    stl_gnf: Dict[int, float] = dataclasses.field(default_factory=dict)\n",
    "    mtl_gnf: Dict[int, Dict[str, float]] = dataclasses.field(default_factory=dict) \n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Features():\n",
    "    values_names = None\n",
    "    names = None\n",
    "    categorical = None\n",
    "    numerical = None\n",
    "    categorical_indices = None\n",
    "    numerical_indices = None\n",
    "    dummy = None\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Config:\n",
    "    def __init__(self, folders: Folders, files: Files, training: Training, tuning: Tuning):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.training = training\n",
    "        self.tuning = tuning\n",
    "        self.folders = folders\n",
    "        self.files = files\n",
    "        self.models = Models(self.folders.base)\n",
    "        self.paths = Paths(\n",
    "            data=self.folders.dataset+self.files.data,\n",
    "            targets=self.folders.dataset+self.files.targets,\n",
    "            study=self.folders.study+self.files.study\n",
    "        )\n",
    "        self.input_size = None\n",
    "        self.results = Results()\n",
    "        self.alpha_list = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "        self.features = Features()\n",
    "        self.best_parameters = None\n",
    "        self.num = 5\n",
    "        self.metrics_label = ''\n",
    "\n",
    "DIRECTION = Literal['max', 'min']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalFidelity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalFidelity, self).__init__()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        loss = torch.mean((predictions - targets) ** 2)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logarithm of the hyperbolic cosine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogHCos(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogHCos, self).__init__()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        loss = torch.mean(torch.log(torch.cosh(predictions-targets)))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccuracyScore(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AccuracyScore, self).__init__()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        loss = (predictions == targets).count_nonzero() / predictions.shape[0]\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glass-box Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black-box Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models were defined in a way that allows for their subsequent optimization with respect to parameters such as the number of hidden layers and the number of neurons in each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_cls(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size=1):\n",
    "        super(MLP_cls, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        last_size = input_size\n",
    "        for hidden in hidden_sizes:\n",
    "            self.hidden_layers.append(nn.Linear(last_size, hidden))\n",
    "            last_size = hidden\n",
    "        self.out_layer = nn.Linear(last_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "            x = self.relu(x)\n",
    "        x = self.out_layer(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def get_hidden_sizes(best_params, config: Config):    \n",
    "        hidden_sizes = [best_params[f'{config.tuning.name_neurons}{i}'] for i in range(best_params[f'{config.tuning.name_layers}'])]\n",
    "        return hidden_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_reg(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size=1):\n",
    "        super(MLP_reg, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        last_size = input_size\n",
    "        for hidden in hidden_sizes:\n",
    "            self.hidden_layers.append(nn.Linear(last_size, hidden))\n",
    "            last_size = hidden\n",
    "        self.out_layer = nn.Linear(last_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "            x = self.relu(x)\n",
    "        x = self.out_layer(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def get_hidden_sizes(best_params, config: Config):    \n",
    "        hidden_sizes = [best_params[f'{config.tuning.name_neurons}{i}'] for i in range(best_params[f'{config.tuning.name_layers}'])]\n",
    "        return hidden_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-Task Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloader, model, optimizer, criterion, device, **kwargs):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for data, target in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        data, target = data.to(device), target.view(-1, 1).to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_reg(data, target, model, criterion, device, **kwargs):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation function for classification tasks includes rounding the output to ensure the values are 0 or 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cls(data, target, model, criterion, device, **kwargs):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(data)\n",
    "        output = (output >= 0.5).float()\n",
    "        loss = criterion(output, target)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with early stopping criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to train the model with a specific early stopping criterion. The function evaluates the model's results on the validation set to halt training at the optimal moment, achieving the best possible results and simultaneously preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __get_worst_value(direction):\n",
    "    if direction == 'max':\n",
    "        return torch.tensor(0)\n",
    "    elif direction == 'min':\n",
    "        return torch.tensor(np.finfo(float).max)\n",
    "\n",
    "def __get_compare_func(direction):\n",
    "    if direction == 'max':\n",
    "        return float.__gt__\n",
    "    elif direction == 'min':\n",
    "        return float.__lt__\n",
    "    \n",
    "\n",
    "def train_with_early_stopping(data_train, target_train, data_eval, target_eval, model, criterion_train, \n",
    "               criterion_eval, eval_func, direction: DIRECTION, config: Config, \n",
    "               num_epoch=None, patience=None, **kwargs):\n",
    "    \n",
    "    assert direction in get_args(DIRECTION), f\"Direction should be one of: {get_args(DIRECTION)}, currently is {direction}\"\n",
    "    if num_epoch == None:\n",
    "        num_epoch = config.training.num_epochs\n",
    "    if patience == None:\n",
    "        patience = config.training.patience\n",
    "\n",
    "    train_dataset = TensorDataset(data_train, target_train)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config.training.batch_size, shuffle=True)\n",
    "    optimizer = config.training.optimizer(model.parameters(), lr=config.training.lr)\n",
    "\n",
    "    best_result = __get_worst_value(direction)\n",
    "    compare = __get_compare_func(direction)\n",
    "    epochs_with_no_improvement = 0\n",
    "\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        train_model(\n",
    "            dataloader=train_dataloader, \n",
    "            model=model, \n",
    "            optimizer=optimizer, \n",
    "            criterion=criterion_train,\n",
    "            device=config.device\n",
    "        )\n",
    "\n",
    "        result = eval_func(\n",
    "            data=data_eval, \n",
    "            target=target_eval, \n",
    "            model=model, \n",
    "            criterion=criterion_eval, \n",
    "            device=config.device\n",
    "        )\n",
    "        \n",
    "        if compare(result.item(), best_result.item()):\n",
    "            best_result = result\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            epochs_with_no_improvement = 0\n",
    "        else:\n",
    "            epochs_with_no_improvement += 1\n",
    "        \n",
    "        if epoch % config.training.epoch_log == 0:\n",
    "            print(f\"Epoch: {epoch}, result ({criterion_eval._get_name()}): {result}\")\n",
    "\n",
    "        if epochs_with_no_improvement >= patience:\n",
    "            print(f\"Early stopping on epoch: {epoch}, best results from epoch: {epoch-patience} result ({criterion_eval._get_name()}): {best_result}\")\n",
    "            model.load_state_dict(best_model)\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Training ended on maximum epoch: {num_epoch}\")\n",
    "        \n",
    "    return best_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for loading, or training and saving models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_train(func, model_path, model, overwrite=False, **kwargs):\n",
    "    if overwrite:\n",
    "        func(model=model, **kwargs)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    else:\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            print(f'Model {model_path} loaded successfully')\n",
    "        except:\n",
    "            print(f'Model {model_path} cannot be loaded. Training has started...')\n",
    "            func(model=model, **kwargs)\n",
    "            torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Conducting the Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to perform the training and testing process a specified number of times (default is 5). The collected results are stored in the configuration. Specifically:\n",
    "\n",
    "- For regression:\n",
    "\n",
    "    - Multilayer Perceptron (MLP)\n",
    "    - Linear Regression\n",
    "    - Single Task Learning (MLP and Linear Regression)\n",
    "\n",
    "- For classification:\n",
    "    - MLP\n",
    "    - Logistic Regression\n",
    "    - Single Task Learning (MLP and Logistic Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_mlp_reg(X_train, y_train, X_eval, y_eval, X_test, y_test, config: Config, num=None):\n",
    "    if num == None:\n",
    "        num = config.num\n",
    "        \n",
    "    for i in range(num):\n",
    "        model = MLP_reg(input_size=config.input_size, hidden_sizes=MLP_reg.get_hidden_sizes(config.best_parameters, config), output_size=1)\n",
    "        load_or_train(\n",
    "            func=train_with_early_stopping, \n",
    "            model_path=config.models.mlp_opt + f'{str(i)}' + config.models.ext,\n",
    "            model=model,\n",
    "            data_train=X_train,\n",
    "            target_train=y_train,\n",
    "            data_eval=X_eval,\n",
    "            target_eval=y_eval,\n",
    "            criterion_train=LogHCos(),\n",
    "            criterion_eval=MSELoss(),\n",
    "            eval_func=eval_reg,\n",
    "            direction='min',\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "        config.results.stl_mlp[i]=eval_reg(\n",
    "            X_test,\n",
    "            y_test,\n",
    "            model,\n",
    "            MSELoss(), \n",
    "            config.device\n",
    "        ).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_lin_reg(X_train, y_train, X_eval, y_eval, X_test, y_test, config: Config, num=None):\n",
    "    if num == None:\n",
    "        num = config.num\n",
    "    for i in range(num):\n",
    "        model = LinearRegression(config.input_size)\n",
    "\n",
    "        load_or_train(\n",
    "            func=train_with_early_stopping, \n",
    "            model_path=config.models.reg + f'{str(i)}' + config.models.ext,\n",
    "            model=model,\n",
    "            data_train=X_train,\n",
    "            target_train=y_train,\n",
    "            data_eval=X_eval,\n",
    "            target_eval=y_eval,\n",
    "            criterion_train=LogHCos(),\n",
    "            criterion_eval=MSELoss(),\n",
    "            eval_func=eval_reg,\n",
    "            direction='min',\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "        config.results.reg[i] = eval_reg(\n",
    "            data=X_test,\n",
    "            target=y_test,\n",
    "            model=model,\n",
    "            criterion=MSELoss(), \n",
    "            device=config.device\n",
    "        ).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_stl_lin_reg(X_train, X_eval, X_test, config: Config, num=None):\n",
    "    if num == None:\n",
    "        num = config.num\n",
    "    for i in range(num):\n",
    "        mlp = MLP_reg(input_size=config.input_size, hidden_sizes=MLP_reg.get_hidden_sizes(config.best_parameters, config)).to(config.device)\n",
    "        mlp.load_state_dict(torch.load(config.models.mlp_opt + str(i) + config.models.ext))\n",
    "        reg = LinearRegression(input_size=config.input_size).to(config.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred_train = mlp(X_train.to(config.device))\n",
    "            y_pred_eval = mlp(X_eval.to(config.device))\n",
    "            y_pred_test = mlp(X_test.to(config.device))\n",
    "\n",
    "        load_or_train(\n",
    "            func=train_with_early_stopping, \n",
    "            model_path=config.models.stl_reg + str(i) + config.models.ext,\n",
    "            model=reg,\n",
    "            data_train=X_train,\n",
    "            target_train=y_pred_train,\n",
    "            data_eval=X_eval,\n",
    "            target_eval=y_pred_eval,\n",
    "            config=config,\n",
    "            criterion_train=GlobalFidelity(),\n",
    "            criterion_eval=GlobalFidelity(),\n",
    "            eval_func=eval_reg,\n",
    "            direction='min'\n",
    "        )\n",
    "\n",
    "        config.results.stl_reg[i] = eval_reg(\n",
    "            data=X_test,\n",
    "            target=y_pred_test,\n",
    "            model=reg,\n",
    "            criterion=GlobalFidelity(), \n",
    "            device=config.device\n",
    "        ).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_test_mlp_cls(X_train, y_train, X_eval, y_eval, X_test, y_test, config: Config, num=None):\n",
    "    if num == None:\n",
    "        num = config.num\n",
    "    for i in range(num):\n",
    "        model = MLP_cls(input_size=config.input_size, hidden_sizes=MLP_cls.get_hidden_sizes(config.best_parameters, config), output_size=1)\n",
    "        load_or_train(\n",
    "            func=train_with_early_stopping, \n",
    "            model_path=config.models.mlp_opt + f'{str(i)}' + config.models.ext,\n",
    "            model=model,\n",
    "            data_train=X_train,\n",
    "            target_train=y_train,\n",
    "            data_eval=X_eval,\n",
    "            target_eval=y_eval,\n",
    "            criterion_train=BCELoss(),\n",
    "            criterion_eval=AccuracyScore(),\n",
    "            eval_func=eval_cls,\n",
    "            direction='max',\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "        config.results.stl_mlp[i]=eval_cls(\n",
    "            X_test,\n",
    "            y_test,\n",
    "            model,\n",
    "            AccuracyScore(), \n",
    "            config.device\n",
    "        ).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_test_lin_cls(X_train, y_train, X_eval, y_eval, X_test, y_test, config: Config, num=None):\n",
    "    if num == None:\n",
    "        num = config.num\n",
    "    for i in range(num):\n",
    "        model = LogisticRegression(config.input_size)\n",
    "\n",
    "        load_or_train(\n",
    "            func=train_with_early_stopping, \n",
    "            model_path=config.models.reg + f'{str(i)}' + config.models.ext,\n",
    "            model=model,\n",
    "            data_train=X_train,\n",
    "            target_train=y_train,\n",
    "            data_eval=X_eval,\n",
    "            target_eval=y_eval,\n",
    "            criterion_train=BCELoss(),\n",
    "            criterion_eval=AccuracyScore(),\n",
    "            eval_func=eval_cls,\n",
    "            direction='max',\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "        config.results.reg[i] = eval_cls(\n",
    "            data=X_test,\n",
    "            target=y_test,\n",
    "            model=model,\n",
    "            criterion=AccuracyScore(), \n",
    "            device=config.device\n",
    "        ).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_stl_lin_cls(X_train, X_eval, X_test, config: Config, num=None):\n",
    "    if num == None:\n",
    "        num = config.num\n",
    "    for i in range(num):\n",
    "        mlp = MLP_cls(input_size=config.input_size, hidden_sizes=MLP_cls.get_hidden_sizes(config.best_parameters, config)).to(config.device)\n",
    "        mlp.load_state_dict(torch.load(config.models.mlp_opt + str(i) + config.models.ext))\n",
    "        reg = LogisticRegression(input_size=config.input_size).to(config.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred_train = (mlp(X_train.to(config.device)) >= 0.5).float()\n",
    "            y_pred_eval = (mlp(X_eval.to(config.device)) >= 0.5).float()\n",
    "            y_pred_test = (mlp(X_test.to(config.device)) >= 0.5).float()\n",
    "\n",
    "        load_or_train(\n",
    "            func=train_with_early_stopping, \n",
    "            model_path=config.models.stl_reg + str(i) + config.models.ext,\n",
    "            model=reg,\n",
    "            data_train=X_train,\n",
    "            target_train=y_pred_train,\n",
    "            data_eval=X_eval,\n",
    "            target_eval=y_pred_eval,\n",
    "            config=config,\n",
    "            criterion_train=GlobalFidelity(),\n",
    "            criterion_eval=GlobalFidelity(),\n",
    "            eval_func=eval_cls,\n",
    "            direction='min'\n",
    "        )\n",
    "\n",
    "        config.results.stl_reg[i] = eval_cls(\n",
    "            data=X_test,\n",
    "            target=y_pred_test,\n",
    "            model=reg,\n",
    "            criterion=GlobalFidelity(), \n",
    "            device=config.device\n",
    "        ).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Task Learning approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multi-task learning, two tasks were specified: the first is a predictive task based on a black-box model (multilayer perceptron), and the second is an explainability task using a linear surrogate model. The soft parameter sharing approach was applied, characterized by a separate set of parameters (weights) for each task and the absence of shared layers.  Communication between tasks occurs through information flow mechanisms; in the discussed example, this is achieved via a shared loss function, which is a convex combination of the neural network's and the surrogate model's loss functions. (The dependencies between the loss functions are regularized by the $\\alpha $ parameter.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTL_mlp_linear(nn.Module):\n",
    "    def __init__(self, mlp, linear):\n",
    "        super(MTL_mlp_linear, self).__init__()\n",
    "        self.mlp = mlp\n",
    "        self.linear = linear\n",
    "\n",
    "    def forward(self, x):\n",
    "        mlp_output = self.mlp(x)\n",
    "        linear_output = self.linear(x)\n",
    "        return mlp_output, linear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the surrogate model, the global fidelity metric was used, whereas the neural network model was assessed using either mean squared error loss or accuracy score. The former for regression tasks and the other for classifications. It is also worth mentioning that global fidelity for classification was calculated using rounded outputs (only values 0 or 1) for both the surrogate and the MLP models. (It might be worth further analysis to consider the fidelity results for probabilities returned directly by the sigmoid function in both models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mtl_model(dataloader, model, optimizer, criterion_mlp, criterion_reg, alpha, device, **kwargs):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for data, target in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        data, target = data.to(device), target.view(-1, 1).to(device)\n",
    "        mlp_output, reg_output = model(data)\n",
    "        loss_mlp = criterion_mlp(mlp_output, target.view(-1, 1))\n",
    "        loss_reg = criterion_reg(reg_output, mlp_output)\n",
    "        loss = alpha * loss_mlp + (1-alpha) * loss_reg\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mtl_cls(data, targets, model, accuracy, fidelity, device):\n",
    "    model = model.to(device)\n",
    "    data, targets = data.to(device), targets.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_mlp, pred_reg = model(data)\n",
    "        pred_mlp = (pred_mlp >= 0.5).float()\n",
    "        pred_reg = (pred_reg >= 0.5).float()\n",
    "    acc_score = accuracy(pred_mlp, targets)\n",
    "    fid_score = fidelity(pred_reg, pred_mlp)\n",
    "    return acc_score.item(), fid_score.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mtl_reg(data, targets, model, mse, fidelity, device):\n",
    "    model = model.to(device)\n",
    "    data, targets = data.to(device), targets.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_mlp, pred_reg = model(data)\n",
    "    mse_score = mse(pred_mlp, targets)\n",
    "    fid_score = fidelity(pred_reg, pred_mlp)\n",
    "    return mse_score.item(), fid_score.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with early stopping criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the STL approach, evaluating whether to apply the early stopping criterion in MTL is somewhat more complex. This complexity arises because the model evaluation function simultaneously returns two values: the result of the neural network and the result of the surrogate model. Therefore, I applied a convex combination similar to the one in training function, based on the same parameter alpha. Ipso facto I obtained a model that performs optimally on both tasks, while maintaining the proportion of importance established during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mtl_early_stopping_cls(data_train, target_train, data_eval, target_eval, model, alpha, config: Config, **kwargs):\n",
    "    \n",
    "    train_dataset = TensorDataset(data_train, target_train)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config.training.batch_size, shuffle=True)\n",
    "    optimizer = config.training.optimizer(model.parameters(), lr=config.training.lr)\n",
    "\n",
    "    best_acc = 0\n",
    "    best_fid = np.finfo(float).max\n",
    "    best_result = np.finfo(float).min\n",
    "    epochs_with_no_improvement = 0\n",
    "\n",
    "    bce_loss = BCELoss()\n",
    "    global_fidelity = GlobalFidelity()\n",
    "    accuracy = AccuracyScore()\n",
    "\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(config.training.num_epochs):\n",
    "        train_mtl_model( \n",
    "            dataloader=train_dataloader, \n",
    "            model=model,\n",
    "            optimizer=optimizer, \n",
    "            criterion_mlp=bce_loss,\n",
    "            criterion_reg=global_fidelity,\n",
    "            alpha=alpha,\n",
    "            device=config.device\n",
    "        )\n",
    "\n",
    "        acc, fid = eval_mtl_cls(\n",
    "            data=data_eval, \n",
    "            targets=target_eval, \n",
    "            model=model,\n",
    "            accuracy=accuracy,\n",
    "            fidelity=global_fidelity, \n",
    "            device=config.device\n",
    "        )\n",
    "\n",
    "        current_result = alpha * acc - (1-alpha) * fid\n",
    "        if current_result > best_result:\n",
    "            best_result = current_result\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            epochs_with_no_improvement = 0\n",
    "            best_acc, best_fid = acc, fid\n",
    "        else:\n",
    "            epochs_with_no_improvement += 1\n",
    "        \n",
    "        if epoch % config.training.epoch_log == 0:\n",
    "            print(f\"Epoch: {epoch}, acc: {acc}, fid: {fid}\")\n",
    "\n",
    "        if epochs_with_no_improvement >= config.training.patience:\n",
    "            print(f\"Early stopping on epoch: {epoch}, best results from epoch: {epoch-config.training.patience}\")\n",
    "            print(f\"Accuracy: {best_acc}, Fidelity: {best_fid}\")\n",
    "            model.load_state_dict(best_model)\n",
    "            break\n",
    "        \n",
    "    return best_acc, best_fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mtl_early_stopping_reg(data_train, target_train, data_eval, target_eval, model, alpha, config: Config, **kwargs):\n",
    "    \n",
    "    train_dataset = TensorDataset(data_train, target_train)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config.training.batch_size, shuffle=True)\n",
    "    optimizer = config.training.optimizer(model.parameters(), lr=config.training.lr)\n",
    "\n",
    "    best_mse = np.finfo(float).max\n",
    "    best_fid = np.finfo(float).max\n",
    "    best_result = np.finfo(float).max\n",
    "    epochs_with_no_improvement = 0\n",
    "\n",
    "    loghcos = LogHCos()\n",
    "    fidelity = GlobalFidelity()\n",
    "    mse_loss = MSELoss()\n",
    "\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(config.training.num_epochs):\n",
    "        train_mtl_model( \n",
    "            dataloader=train_dataloader, \n",
    "            model=model,\n",
    "            optimizer=optimizer, \n",
    "            criterion_mlp=loghcos,\n",
    "            criterion_reg=fidelity,\n",
    "            alpha=alpha,\n",
    "            device=config.device\n",
    "        )\n",
    "\n",
    "        mse, fid = eval_mtl_reg(\n",
    "            data=data_eval, \n",
    "            targets=target_eval, \n",
    "            model=model,\n",
    "            mse=mse_loss,\n",
    "            fidelity=fidelity,\n",
    "            device=config.device\n",
    "        )\n",
    "\n",
    "        current_result = alpha * mse + (1-alpha) * fid\n",
    "        if current_result < best_result:\n",
    "            best_result = current_result\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            epochs_with_no_improvement = 0\n",
    "            best_mse, best_fid = mse, fid\n",
    "        else:\n",
    "            epochs_with_no_improvement += 1\n",
    "        \n",
    "        if epoch % config.training.epoch_log == 0:\n",
    "            print(f\"Epoch: {epoch}, mse: {mse}, fid: {fid}\")\n",
    "\n",
    "        if epochs_with_no_improvement >= config.training.patience:\n",
    "            print(f\"Early stopping on epoch: {epoch}, best results from epoch: {epoch-config.training.patience}\")\n",
    "            print(f\"MSE: {best_mse}, Fidelity: {best_fid}\")\n",
    "            model.load_state_dict(best_model)\n",
    "            break\n",
    "        \n",
    "    return best_mse, best_fid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Conducting the Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for training and testing for given alpha parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_mtl_cls(data_train, data_eval, data_test, target_train, target_eval, target_test, alpha, config: Config, path=None, save_load=False, model_params=None):\n",
    "    if not model_params: model_params = config.best_parameters\n",
    "    \n",
    "    mlp = MLP_cls(input_size=config.input_size, hidden_sizes=MLP_cls.get_hidden_sizes(model_params, config))\n",
    "    reg = LogisticRegression(input_size=config.input_size)\n",
    "    model = MTL_mlp_linear(mlp=mlp, linear=reg)\n",
    "\n",
    "    if save_load:\n",
    "        load_or_train(\n",
    "            func=train_mtl_early_stopping_cls, \n",
    "            model=model,\n",
    "            model_path=path,\n",
    "            data_train=data_train,\n",
    "            data_eval=data_eval,\n",
    "            target_train=target_train,\n",
    "            target_eval=target_eval,\n",
    "            alpha=alpha,\n",
    "            config=config\n",
    "        )\n",
    "    else:\n",
    "        train_mtl_early_stopping_cls(\n",
    "            data_train=data_train, \n",
    "            target_train=target_train, \n",
    "            data_eval=data_eval, \n",
    "            target_eval=target_eval, \n",
    "            model=model, \n",
    "            alpha=alpha, \n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "    acc, fid = eval_mtl_cls(\n",
    "        data=data_test, \n",
    "        targets=target_test, \n",
    "        model=model, \n",
    "        device=config.device,\n",
    "        accuracy=AccuracyScore(),\n",
    "        fidelity=GlobalFidelity()\n",
    "    )\n",
    "    return acc, fid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_mtl_reg(data_train, data_eval, data_test, target_train, target_eval, target_test, alpha, config: Config, path=None, save_load=False, model_params=None):\n",
    "    if not model_params: model_params = config.best_parameters\n",
    "    \n",
    "    mlp = MLP_reg(input_size=config.input_size, hidden_sizes=MLP_reg.get_hidden_sizes(model_params, config))\n",
    "    lin = LinearRegression(input_size=config.input_size)\n",
    "    model = MTL_mlp_linear(mlp=mlp, linear=lin)\n",
    "\n",
    "    if save_load:\n",
    "        load_or_train(\n",
    "            func=train_mtl_early_stopping_reg, \n",
    "            model=model,\n",
    "            model_path=path,\n",
    "            data_train=data_train,\n",
    "            data_eval=data_eval,\n",
    "            target_train=target_train,\n",
    "            target_eval=target_eval,\n",
    "            alpha=alpha,\n",
    "            config=config\n",
    "        )\n",
    "    else:\n",
    "        train_mtl_early_stopping_reg(\n",
    "            data_train=data_train, \n",
    "            target_train=target_train, \n",
    "            data_eval=data_eval, \n",
    "            target_eval=target_eval, \n",
    "            model=model, \n",
    "            alpha=alpha, \n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "    mse, fid = eval_mtl_reg(\n",
    "        data=data_test, \n",
    "        targets=target_test, \n",
    "        model=model, \n",
    "        device=config.device,\n",
    "        mse=MSELoss(),\n",
    "        fidelity=GlobalFidelity()\n",
    "    )\n",
    "    return mse, fid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for training and testing models for list of alpha (default values from a closed interval [0, 1] with a step of 0.1). Acquired results are stored in config.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_mtl_cls_for_alpha_list(data_train, data_eval, data_test, \n",
    "                           target_train, target_eval, target_test, \n",
    "                           num, config: Config, alpha_list=None, model_params=None):\n",
    "    \n",
    "    if not alpha_list: alpha_list = config.alpha_list\n",
    "    config.results.mtl[num] = {}\n",
    "\n",
    "    for alpha in alpha_list:\n",
    "        print(f'Model for alpha: {alpha}')\n",
    "        path = config.models.mtl + str(alpha) + '_' + str(num) + config.models.ext\n",
    "        acc, fid = train_and_test_mtl_cls(\n",
    "            data_train=data_train,\n",
    "            data_eval=data_eval,\n",
    "            data_test=data_test,\n",
    "            target_train=target_train,\n",
    "            target_eval=target_eval,\n",
    "            target_test=target_test,\n",
    "            alpha=alpha,\n",
    "            path=path,\n",
    "            save_load=True,\n",
    "            config=config\n",
    "        )\n",
    "        print(f'Result on test dataset: {path}, Accuracy: {acc}, Fidelity: {fid}')\n",
    "        config.results.mtl[num][str(alpha)] = Result(acc, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_mtl_reg_for_alpha_list(data_train, data_eval, data_test, \n",
    "                           target_train, target_eval, target_test, \n",
    "                           num, config: Config, alpha_list=None, model_params=None):\n",
    "    \n",
    "    if not alpha_list: alpha_list = config.alpha_list\n",
    "    config.results.mtl[num] = {}\n",
    "\n",
    "    for alpha in alpha_list:\n",
    "        print(f'Model for alpha: {alpha}')\n",
    "        path = config.models.mtl + str(alpha) + '_' + str(num) + config.models.ext\n",
    "        mse, fid = train_and_test_mtl_reg(\n",
    "            data_train=data_train,\n",
    "            data_eval=data_eval,\n",
    "            data_test=data_test,\n",
    "            target_train=target_train,\n",
    "            target_eval=target_eval,\n",
    "            target_test=target_test,\n",
    "            alpha=alpha,\n",
    "            path=path,\n",
    "            save_load=True,\n",
    "            config=config\n",
    "        )\n",
    "        print(f'Result on test dataset: {path}, MSE: {mse}, Fidelity: {fid}')\n",
    "        config.results.mtl[num][str(alpha)] = Result(mse, fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to perform the training and testing process a specified number of times (default is 5). The collected results are stored in the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_mtl_reg_n_times(data_train, target_train, data_eval, target_eval, data_test, target_test, config: Config, num=None):\n",
    "    if num == None:\n",
    "        num = config.num\n",
    "    for i in range(num):\n",
    "        train_and_test_mtl_reg_for_alpha_list(\n",
    "            data_train=data_train,\n",
    "            data_eval=data_eval,\n",
    "            data_test=data_test,\n",
    "            target_train=target_train,\n",
    "            target_eval=target_eval,\n",
    "            target_test=target_test,\n",
    "            config=config,\n",
    "            num=i\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_mtl_cls_n_times(data_train, target_train, data_eval, target_eval, data_test, target_test, config: Config, num=None):\n",
    "    if num == None:\n",
    "        num = config.num\n",
    "    for i in range(num):\n",
    "        train_and_test_mtl_cls_for_alpha_list(\n",
    "            data_train=data_train,\n",
    "            data_eval=data_eval,\n",
    "            data_test=data_test,\n",
    "            target_train=target_train,\n",
    "            target_eval=target_eval,\n",
    "            target_test=target_test,\n",
    "            config=config,\n",
    "            num=i\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization is conducted using the Optuna library. Parameters that were optimized:\n",
    "- number of hidden layers (ranging from 1 to 5),\n",
    "- number of neurons per layer (ranging from one-quater to four times the number of input features).\n",
    "\n",
    "Other parameters such as optimizer or learning rate were taken from the discussed reasearch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading function \n",
    "\n",
    "Other optimization functions are defined individually for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_load_study(func, config: Config, overwrite = False):\n",
    "    if overwrite:\n",
    "        return func(config)\n",
    "    try:\n",
    "        study = joblib.load(config.paths.study)\n",
    "    except:\n",
    "        study = func(config)\n",
    "    return study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function presenting six plots illustrating the following relationships:\n",
    "\n",
    "- The relationship between the black-box model evaluation metric and the alpha parameter value.\n",
    "- The relationship between the surrogate model evaluation metric (global fidelity), and the alpha parameter value.\n",
    "- The comparison of results for the STL (Single-Task Learning) and MTL (Multi-Task Learning) approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_plots_MTL_vs_STL(metrics_label, metrics_title, config: Config, org_met=None, org_fid=None, title=None):\n",
    "    COLORS = ['lightcoral', 'red', 'peru', 'moccasin', 'gold', 'greenyellow', 'lime', 'turquoise', 'skyblue', 'royalblue', 'violet', 'black']\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(12, 18))\n",
    "\n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=18)\n",
    "\n",
    "    metrics_score_list = []\n",
    "    fid_score_list = []\n",
    "\n",
    "    for alpha in config.alpha_list:\n",
    "        temp_metric = []\n",
    "        temp_fid = []\n",
    "        for i in range(config.num):\n",
    "            temp_metric.append(config.results.mtl[i][str(alpha)].metrics)\n",
    "            temp_fid.append(config.results.mtl[i][str(alpha)].fid)\n",
    "        metrics_score_list.append(np.mean(temp_metric))\n",
    "        fid_score_list.append(np.mean(temp_fid))\n",
    "\n",
    "    alpha_list = config.alpha_list\n",
    "    stl_metrics_score = np.mean(list(config.results.stl_mlp.values()))\n",
    "    stl_fid_score = np.mean(list(config.results.stl_reg.values()))\n",
    "\n",
    "    # MSE/alpha\n",
    "    axes[0,0].plot(metrics_score_list, alpha_list, marker='o')\n",
    "    axes[0,0].set_ylabel('alpha')\n",
    "    axes[0,0].set_yticks(ticks=alpha_list)\n",
    "    axes[0,0].set_xlabel(metrics_label)\n",
    "    axes[0,0].set_title(label=f'{metrics_label}/alpha[0, 1]')\n",
    "    axes[0,0].text(-0.1, 1.05, '(1)', ha='left', va='top', fontsize=12, transform=axes[0,0].transAxes)\n",
    "\n",
    "    axes[0,1].plot(metrics_score_list[1:-1], alpha_list[1:-1], marker='o')\n",
    "    axes[0,1].set_ylabel('alpha')\n",
    "    axes[0,1].set_yticks(ticks=alpha_list[1:-1])\n",
    "    axes[0,1].set_xlabel(metrics_label)\n",
    "    axes[0,1].set_title(label=f'{metrics_label}/alpha(0, 1)')\n",
    "    axes[0,1].xaxis.set_major_locator(plt.MaxNLocator(7))\n",
    "    axes[0,1].text(-0.1, 1.05, '(2)', ha='left', va='top', fontsize=12, transform=axes[0,1].transAxes)\n",
    "\n",
    "\n",
    "    if org_met:\n",
    "        axes[0,1].plot(org_met, alpha_list[1:-1], marker='o', color='darkviolet')\n",
    "        axes[0,1].legend(labels=('own', 'orig.'))\n",
    "\n",
    "\n",
    "    # Fidelity/alpha\n",
    "    axes[1,0].plot(fid_score_list, alpha_list, marker='o')\n",
    "    axes[1,0].set_ylabel('alpha')\n",
    "    axes[1,0].set_yticks(ticks=alpha_list)\n",
    "    axes[1,0].set_xlabel('fidelity')\n",
    "    axes[1,0].set_title(label='Fidelity/alpha[0, 1]')\n",
    "    axes[1,0].text(-0.1, 1.05, '(3)', ha='left', va='top', fontsize=12, transform=axes[1,0].transAxes)\n",
    "\n",
    "\n",
    "    axes[1,1].plot(fid_score_list[1:-1], alpha_list[1:-1], marker='o')\n",
    "    axes[1,1].set_ylabel('alpha')\n",
    "    axes[1,1].set_yticks(ticks=alpha_list[1:-1])\n",
    "    axes[1,1].set_xlabel('fidelity')\n",
    "    axes[1,1].set_title(label='Fidelity/alpha(0, 1)')\n",
    "    axes[1,1].xaxis.set_major_locator(plt.MaxNLocator(7))\n",
    "    axes[1,1].text(-0.1, 1.05, '(4)', ha='left', va='top', fontsize=12, transform=axes[1,1].transAxes)\n",
    "\n",
    "\n",
    "    if org_fid:\n",
    "        axes[1,1].plot(org_fid, alpha_list[1:-1], marker='o', color='darkviolet')\n",
    "        axes[1,1].legend(labels=('own', 'orig.'))\n",
    "\n",
    "    # MSE/Fidelity alpha [0, 1]\n",
    "    metrics_score_list_ex = metrics_score_list + [stl_metrics_score]\n",
    "    fid_score_list_ex = fid_score_list + [stl_fid_score]\n",
    "    alpha = [f'{chr(945)}={alpha}' for alpha in alpha_list]+['STL    ']\n",
    "    alpha_legend = alpha\n",
    "    legend_handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=15) for color in COLORS]\n",
    "    legend = [f\"{alpha} ({'{:.4f}'.format(round(float(metrics), 4))} / {'{:.4f}'.format(round(fid, 4))})\" for alpha, metrics, fid in zip(alpha_legend, metrics_score_list_ex, fid_score_list_ex)]\n",
    "\n",
    "    linear_met = np.mean(list(config.results.reg.values()))\n",
    "\n",
    "    axes[2,0].scatter(metrics_score_list_ex, fid_score_list_ex, marker='o', s=100, edgecolors='black', c=COLORS)\n",
    "    axes[2,0].set_title(f'{metrics_title}/Fidelity (with STL)')\n",
    "    axes[2,0].legend(legend_handles, legend)\n",
    "    axes[2,0].set_ylabel('fidelity')\n",
    "    axes[2,0].set_xlabel(metrics_label)\n",
    "    axes[2,0].axvline(x = linear_met, color = 'seagreen', label = 'linear')\n",
    "    axes[2,0].text(linear_met, max(fid_score_list_ex), 'linear' + f\" {metrics_label}: {linear_met:.4f}\", color='seagreen', ha='right', va='top', rotation=90, style='italic')\n",
    "    axes[2,0].text(-0.1, 1.05, '(5)', ha='left', va='top', fontsize=12, transform=axes[2,0].transAxes)\n",
    "\n",
    "\n",
    "    # MSE/Fidelity alpha (0, 1)\n",
    "    metrics_score_list_ex = [round(float(x), 4) for x in metrics_score_list[1:-1] + [stl_metrics_score]]\n",
    "    fid_score_list_ex = [float(fid) for fid in fid_score_list[1:-1] + [stl_fid_score]]\n",
    "    alpha = [f'{chr(945)}={alpha}' for alpha in alpha_list[1:-1]]+['STL    ']\n",
    "    colors = COLORS[1:-2] + ['black']\n",
    "    plot_data = [[alpha, metrics, fid, col] for alpha, metrics, fid, col in sorted(zip(alpha, metrics_score_list_ex, fid_score_list_ex, colors), key=lambda l: l[2], reverse=reversed)]\n",
    "\n",
    "    legend_handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=15) for color in [pd[3] for pd in plot_data]]\n",
    "    legend = [f\"{alpha} ({'{:.4f}'.format(round(float(metrics), 4))} / {'{:.4f}'.format(round(fid, 4))})\" for alpha, metrics, fid, _ in plot_data]\n",
    "\n",
    "    axes[2,1].scatter(metrics_score_list_ex, fid_score_list_ex, marker='o', s=100, edgecolors='black', c=colors)\n",
    "    axes[2,1].set_title(f'{metrics_title}/Fidelity (with STL)')\n",
    "    axes[2,1].legend(handles=legend_handles, labels=legend, title=f'alpha ({metrics_label}/fid)', loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    axes[2,1].set_ylabel('fidelity')\n",
    "    axes[2,1].set_xlabel(metrics_label)\n",
    "    axes[2,1].axvline(x = linear_met, color = 'seagreen', label = 'linear')\n",
    "    axes[2,1].text(linear_met, max(fid_score_list_ex), 'linear' + f\" {metrics_label}: {linear_met:.4f}\", color='seagreen', ha='right', va='top', rotation=90, style='italic')\n",
    "    axes[2,1].xaxis.set_major_locator(plt.MaxNLocator(7))\n",
    "    axes[2,1].text(-0.1, 1.05, '(6)', ha='left', va='top', fontsize=12, transform=axes[2,1].transAxes)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(top=0.93, left=0.15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to show MTL results in form of table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_non_lin_vs_lin(config: Config, metrics_label=None):\n",
    "        if metrics_label == None:\n",
    "            metrics_label = config.metrics_label\n",
    "        return pd.DataFrame({\n",
    "            'Metrics': [metrics_label],\n",
    "            'Dataset': [config.folders.base.split('/')[0]],\n",
    "            'Linear': [np.mean(list(config.results.reg.values()))],\n",
    "            'Non-linear (MLP)': [np.mean(list(config.results.stl_mlp.values()))]\n",
    "        })\n",
    "\n",
    "def get_results_mtl_vs_stl(config: Config, metrics_label=None):\n",
    "    if metrics_label == None:\n",
    "            metrics_label = config.metrics_label\n",
    "    df = pd.DataFrame({\n",
    "        'Metrics': [metrics_label, 'Global Fidelity'],\n",
    "        'Dataset': [config.folders.base.split('/')[0], config.folders.base.split('/')[0]],\n",
    "        'Linear': [np.mean(list(config.results.reg.values())), '-'],\n",
    "        'STL': [np.mean(list(config.results.stl_mlp.values())), np.mean(list(config.results.stl_reg.values()))]\n",
    "    })\n",
    "\n",
    "    for alpha in config.alpha_list:\n",
    "        temp_metric = []\n",
    "        temp_fid = []\n",
    "        for i in range(config.num):\n",
    "            temp_metric.append(config.results.mtl[i][str(alpha)].metrics)\n",
    "            temp_fid.append(config.results.mtl[i][str(alpha)].fid)\n",
    "        df[f'MTL a= {str(alpha)}'] = [np.mean(temp_metric), np.mean(temp_fid)]\n",
    "    return df\n",
    "    \n",
    "\n",
    "def show_tables(config: Config, metrics_label=None):\n",
    "    if metrics_label == None:\n",
    "        metrics_label = config.metrics_label\n",
    "    try:\n",
    "        df1 = pd.read_pickle(config.folders.base + 'results_lin_vs_non_lin.pkl')\n",
    "    except:\n",
    "        df1 = get_results_non_lin_vs_lin(config, metrics_label)\n",
    "        df1.to_pickle(config.folders.base + 'results_lin_vs_non_lin.pkl')\n",
    "\n",
    "    display(df1.style.format(precision=4, decimal=\",\").hide())\n",
    "\n",
    "    try:\n",
    "        df2 = pd.read_pickle(config.folders.base + 'results_stl_vs_mtl.pkl')\n",
    "    except:\n",
    "        df2 = get_results_mtl_vs_stl(config, metrics_label)\n",
    "        df2.to_pickle(config.folders.base + 'results_stl_vs_mtl.pkl')\n",
    "\n",
    "    display(df2.style.format(precision=4, decimal=\",\").hide())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to show GNF results in form of table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gnf_df(config: Config):\n",
    "    df = pd.DataFrame({\n",
    "        'Metric': ['GNF'],\n",
    "        'Dataset': [config.folders.base.split('/')[0]],\n",
    "        'Linear': ['-'],\n",
    "        'STL': [np.mean(config.results.stl_gnf)]\n",
    "\n",
    "    })\n",
    "    for alpha in config.alpha_list:\n",
    "        temp_list = []\n",
    "        for i in range(config.num):\n",
    "            temp_list.append(config.results.mtl_gnf[i][str(alpha)])\n",
    "        df[f'MTL a= {str(alpha)}'] = [np.mean(temp_list)]\n",
    "    return df\n",
    "\n",
    "def get_gnf_summary_config(config_list):\n",
    "    df_list = []\n",
    "    for config in config_list:\n",
    "        df_list.append(get_gnf_df(config))\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "def get_prediction_results(config: Config, metrics_label, n=0):\n",
    "    df = pd.DataFrame({\n",
    "        'Metric': [metrics_label],\n",
    "        'Dataset': [config.folders.base.split('/')[0]],\n",
    "        'Linear': [config.results.reg[n]],\n",
    "        'STL': [config.results.stl_mlp[n]]\n",
    "    })\n",
    "    for alpha in config.alpha_list:\n",
    "        df[f'MTL a= {str(alpha)}'] = config.results.mtl[n][str(alpha)].metrics\n",
    "    return df\n",
    "\n",
    "def get_gnf_and_metrics(config: Config, metrics_label=None, num=0):\n",
    "    if metrics_label == None:\n",
    "            metrics_label = config.metrics_label\n",
    "    try:\n",
    "        df = pd.read_pickle(config.folders.base + 'results_gnf.pkl')\n",
    "    except:\n",
    "        df_gnf = get_gnf_df(config)\n",
    "        df_res = get_prediction_results(config, metrics_label, num)\n",
    "        df = pd.concat([df_gnf, df_res])\n",
    "        df.to_pickle(config.folders.base + 'results_gnf.pkl')\n",
    "    return df\n",
    "\n",
    "\n",
    "def show_tables_lime(config: Config, metrics_label=None, num=0):\n",
    "    display(get_gnf_and_metrics(config, metrics_label, num).style.format(precision=3, decimal=\",\").hide())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Explainability with Lime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions of the prediction functions for classification - LIME requires a minimum of two classes; therefore, the prediction function should return two probability values. Also prediction function should include appropriate data transformations, as the models were trained on encoded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _X_to_tensor(X, config:Config):\n",
    "    X = pd.DataFrame(X, columns=config.features.names)\n",
    "    X = pd.get_dummies(X, columns=config.features.categorical, dtype=float)\n",
    "    X = X.reindex(columns=config.features.dummy, fill_value=0.0).values\n",
    "    return torch.tensor(X, dtype=torch.float32).to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cls_for_lime(X, model, config: Config):\n",
    "    X = _X_to_tensor(X=X, config=config)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(X).cpu().numpy()\n",
    "    return np.hstack((1 - pred, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mtl_cls_for_lime(X, model, config: Config):\n",
    "    X = _X_to_tensor(X=X, config=config)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(X)[0].cpu().numpy()\n",
    "    return np.hstack((1 - pred, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions of the prediction functions for regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_reg_for_lime(X, model, config: Config):\n",
    "    X = _X_to_tensor(X=X, config=config)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(X).cpu().numpy()  \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mtl_reg_for_lime(X, model, config: Config):\n",
    "    X = _X_to_tensor(X=X, config=config)\n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        pred = model(X)[0].cpu().numpy()\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighbors Generation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for generating neighbors ($|N_x| = 10$)\n",
    "- For continuous features: using a normal distribution $\\mathcal{N}(ùë•,ùúá,ùúé^2)$\n",
    "where $¬µ=0, œÉ^2=0.1$\n",
    "- For categorical features: lack of information in the discussed study "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STD_DEV = np.sqrt(0.1)\n",
    "def generate_neighbors_continuous_only(instance, config: Config, mean=0, std_deviation=STD_DEV, num_neighbors=10, features_num_idx=None):\n",
    "    if features_num_idx==None:\n",
    "        features_num_idx = config.features.numerical_indices\n",
    "    perturbations = np.random.normal(loc=0, scale=std_deviation, size=(num_neighbors, len(features_num_idx)))\n",
    "    neighbors = np.repeat(instance.reshape(1, -1), num_neighbors, axis=0)\n",
    "    neighbors[:, features_num_idx] += perturbations\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for getting LIME predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lime_prediction_reg(instance, exp):\n",
    "    pred = sum([x[1]*instance[x[0]] for x in exp.local_exp[1]]) + exp.intercept[1]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lime_prediction_cat(instance, exp, categorical_indices):\n",
    "    result = 0\n",
    "    for x in exp.local_exp[1]:\n",
    "        if x[0] in categorical_indices:\n",
    "            result += x[1]\n",
    "        else:\n",
    "            result += x[1]*instance[x[0]]\n",
    "    return result + exp.intercept[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lime_prediction(local_neighbors, exp, categorical_indices):\n",
    "    if categorical_indices:\n",
    "        lime_predictions = torch.tensor([get_lime_prediction_cat(x, exp, categorical_indices) for x in local_neighbors])\n",
    "    else:\n",
    "        lime_predictions = torch.tensor([get_lime_prediction_reg(x, exp) for x in local_neighbors])\n",
    "    return lime_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Neighborhood Fidelity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Neighborhood Fidelity is defined as the averaged value of Neighborhood Fidelity for all data points. Neighborhood Fidelity, in turn, is defined as the value of Global Fidelity in the local neighborhood of a given point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Neighborhood Fidelity for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_neighborhood_fidelity_reg(model, neighbors_dataset, explainer, predict_func_lime, predict_func_model, config: Config, num_neighbors=10, n=1):\n",
    "    result = []\n",
    "    fidelity = GlobalFidelity()\n",
    "    for i in range(n):\n",
    "        fidelity_score = []\n",
    "        for instance in neighbors_dataset:\n",
    "            exp = explainer.explain_instance(instance, predict_func_lime)\n",
    "            local_neighbors = generate_neighbors_continuous_only(instance, config, num_neighbors)\n",
    "            lime_predictions = get_lime_prediction(local_neighbors=local_neighbors, exp=exp, categorical_indices=config.features.categorical_indices)\n",
    "            model_predictions = torch.tensor(np.array([pred for pred in predict_func_model(local_neighbors, model, config)]))\n",
    "            fidelity_score.append(fidelity(lime_predictions, model_predictions))\n",
    "            \n",
    "        result.append(torch.mean(torch.tensor(fidelity_score)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Neighborhood Fidelity for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_neighborhood_fidelity_cls(model, neighbors_dataset, explainer, predict_func_lime, predict_func_model, config: Config, num_neighbors=10, n=1):\n",
    "    result = []\n",
    "    fidelity = GlobalFidelity()\n",
    "    for i in range(n):\n",
    "        fidelity_score = []\n",
    "        for instance in neighbors_dataset:\n",
    "            exp = explainer.explain_instance(instance, predict_func_lime)\n",
    "            local_neighbors = generate_neighbors_continuous_only(instance, config, num_neighbors) \n",
    "            lime_predictions = (get_lime_prediction(local_neighbors=local_neighbors, exp=exp, categorical_indices=config.features.categorical_indices) >= 0.5).float()\n",
    "            model_predictions = (torch.tensor(np.array([pred2 for pred1, pred2 in predict_func_model(local_neighbors, model, config)])) >= 0.5).float()\n",
    "            fidelity_score.append(fidelity(lime_predictions, model_predictions))\n",
    "        result.append(torch.mean(torch.tensor(fidelity_score)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Conducting the Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Neighborhood Fidelity for MTL regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnf_for_mtl_reg(neighbors_dataset, explainer, model_params, path, config: Config, num_neighbors=10):\n",
    "    mlp = MLP_reg(input_size=config.input_size, hidden_sizes=MLP_reg.get_hidden_sizes(model_params, config))\n",
    "    lin = LinearRegression(input_size=config.input_size)\n",
    "    model = MTL_mlp_linear(mlp=mlp, linear=lin).to(config.device)\n",
    "    \n",
    "    predict_func_lime = partial(predict_mtl_reg_for_lime, model=model, config=config)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "    return global_neighborhood_fidelity_reg(\n",
    "        model=model,\n",
    "        neighbors_dataset=neighbors_dataset,\n",
    "        explainer=explainer, \n",
    "        predict_func_lime=predict_func_lime,\n",
    "        predict_func_model=predict_mtl_reg_for_lime,\n",
    "        config=config, \n",
    "        num_neighbors=num_neighbors\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnf_for_mtl_reg_for_alpha_list(neighbors_dataset, explainer, model_params, config: Config, num_neighbors=10, alpha_list=None, n=5):\n",
    "    if not n: n = config.num\n",
    "    if not alpha_list: alpha_list=config.alpha_list\n",
    "    for i in range(n):\n",
    "        config.results.mtl_gnf[i] = {}\n",
    "        for alpha in alpha_list:\n",
    "            path = config.models.mtl + str(alpha) + '_0' + config.models.ext\n",
    "            config.results.mtl_gnf[i][str(alpha)] = gnf_for_mtl_reg(\n",
    "                neighbors_dataset=neighbors_dataset,\n",
    "                explainer=explainer, \n",
    "                model_params=config.best_parameters,\n",
    "                path=path,\n",
    "                config=config, \n",
    "                num_neighbors=num_neighbors\n",
    "            )\n",
    "            print(f'Model: {path}, GNF: {config.results.mtl_gnf[i][str(alpha)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Neighborhood Fidelity for MTL classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnf_for_mtl_cls(neighbors_dataset, explainer, model_params, path, config: Config, num_neighbors=10):\n",
    "    mlp = MLP_cls(input_size=config.input_size, hidden_sizes=MLP_cls.get_hidden_sizes(model_params, config))\n",
    "    lin = LogisticRegression(input_size=config.input_size)\n",
    "    model = MTL_mlp_linear(mlp=mlp, linear=lin).to(config.device)\n",
    "\n",
    "    predict_func_lime = partial(predict_mtl_cls_for_lime, model=model, config=config)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return global_neighborhood_fidelity_cls(\n",
    "        model=model,\n",
    "        neighbors_dataset=neighbors_dataset,\n",
    "        explainer=explainer, \n",
    "        predict_func_lime=predict_func_lime,\n",
    "        predict_func_model=predict_mtl_cls_for_lime,\n",
    "        config=config, \n",
    "        num_neighbors=num_neighbors\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnf_for_mtl_cls_for_alpha_list(neighbors_dataset, explainer, model_params, config: Config, num_neighbors=10, alpha_list=None, n=5):\n",
    "    if not n: n = config.num\n",
    "    if not alpha_list: alpha_list=config.alpha_list\n",
    "    for i in range(n):\n",
    "        config.results.mtl_gnf[i] = {}\n",
    "        for alpha in alpha_list:\n",
    "            path = config.models.mtl + str(alpha) + '_0' + config.models.ext\n",
    "            config.results.mtl_gnf[i][str(alpha)] = gnf_for_mtl_cls(\n",
    "                neighbors_dataset=neighbors_dataset,\n",
    "                explainer=explainer, \n",
    "                model_params=config.best_parameters,\n",
    "                path=path,\n",
    "                config=config, \n",
    "                num_neighbors=num_neighbors\n",
    "            )\n",
    "            print(f'Model: {path}, GNF: {config.results.mtl_gnf[i][str(alpha)]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
